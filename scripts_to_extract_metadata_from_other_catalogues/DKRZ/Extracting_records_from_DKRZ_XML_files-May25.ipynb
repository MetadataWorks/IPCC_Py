{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from nltk import tokenize\n",
    "import re\n",
    "import json\n",
    "import openpyxl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Folder where the xml files are located\n",
    "FILE_DIR = \"Input2025\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reads in the filenames, and prints out how many start with \"oai\"\n",
    "\n",
    "filename_list = []\n",
    "for filename in os.listdir(FILE_DIR):\n",
    "    if filename.startswith('oai'):\n",
    "        filename_list.append(filename)\n",
    "print(len(filename_list))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DKRZ use a number of tags for references.  These variables store the relevant tags against the agreed variable \n",
    "# in the IPCC DDC schema v2\n",
    "\n",
    "references_rel_types = ['IsDocumentedBy', 'Cites', 'IsDescribedBy', 'References']\n",
    "referencedby_rel_types = ['Documents', 'IsCitedBy', 'Describes', 'IsReferencedBy']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML IMPORT AND MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# This script reads in the files and creates a list of python dictionaries, each one containing the mapped \n",
    "# metadata from each XML file\n",
    "\n",
    "import_list = []\n",
    "for xml_file in filename_list:\n",
    "    import_dict = {}\n",
    "    etree = ET.parse(os.path.join(FILE_DIR,xml_file))\n",
    "    root = etree.getroot()\n",
    "    \n",
    "    #### SUMMARY\n",
    "    \n",
    "    # Title\n",
    "    for titles in root.findall('{https://datacite.org/schema/kernel-4}titles'):\n",
    "        import_dict['Title'] = titles.find('{https://datacite.org/schema/kernel-4}title').text\n",
    "    \n",
    "    #Keywords\n",
    "    keyword_list = []\n",
    "    for keywords in root.findall('{http://datacite.org/schema/kernel-4}subjects'):\n",
    "        for keyword in keywords.findall('{http://datacite.org/schema/kernel-4}subject'):\n",
    "            keyword_list.append(keyword.text)\n",
    "    import_dict['Keywords'] = keyword_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    # DOI name, or alternative indentifiers\n",
    "    \n",
    "    '''\n",
    "    For DOIs, DKRZ provided the following detail on how to create resolvable URLS from the DOIs:\n",
    "    \n",
    "    We have persistent urls pointing to the landing page, which provide information about and access to the data. Unfortunately, we haven't these urls in the provided metadata and there are two construction rules based on the 'doi name'.\n",
    "    \n",
    "    special case for doi names with 'CMIP5.' \n",
    "    or which are like '10.1594/WDCC/CMIP5.': http://cera-www.dkrz.de/WDCC/CMIP5/Compact.jsp?acronym= \n",
    "    e.g. 10.1594/WDCC/CMIP5.MXELr4: http://cera-www.dkrz.de/WDCC/CMIP5/Compact.jsp?acronym=MXELr4\n",
    "    \n",
    "    all other cases, currently with 'doi names' like '10.1594/WDCC/ or '10.26050/WDCC/': \n",
    "    http://cera-www.dkrz.de/WDCC/ui/Compact.jsp?acronym= \n",
    "    e.g. '10.1594/WDCC/ETHr8': http://cera-www.dkrz.de/WDCC/ui/Compact.jsp?acronym=10.1594/WDCC/ETHr8\n",
    "    \n",
    "    Alternatively, we could use DOI resolver+DOI name: https://doi.org/\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    for identifier in root.findall('{http://datacite.org/schema/kernel-4}identifier'):\n",
    "        id_type = identifier.get('identifierType')\n",
    "        if id_type == \"DOI\":\n",
    "            import_dict['DOI Name'] = identifier.text\n",
    "            import_dict['Alternate Identifier'] = ''\n",
    "            if identifier.text.startswith('10.1594/WDCC/CMIP5.'):\n",
    "                acronym = identifier.text.split('10.1594/WDCC/CMIP5.', 1)[1]\n",
    "                import_dict['Access URL'] = 'http://cera-www.dkrz.de/WDCC/CMIP5/Compact.jsp?acronym={}'.format(acronym)\n",
    "            else:\n",
    "                acronym = identifier.text.split('/WDCC/', 1)[1]\n",
    "                import_dict['Access URL'] = 'http://cera-www.dkrz.de/WDCC/ui/Compact.jsp?acronym={}'.format(acronym)\n",
    "        else:\n",
    "            import_dict['DOI Name'] = ''\n",
    "            import_dict['Alternate Identifier'] = identifier.text\n",
    "            import_dict['Access URL'] = identifier.text\n",
    "            \n",
    "    # Publication date\n",
    "    for dates in root.findall('{http://datacite.org/schema/kernel-4}dates'):\n",
    "        for date in dates.findall('{http://datacite.org/schema/kernel-4}date'):\n",
    "            date_type = date.get('dateType')\n",
    "            if date_type == \"Created\": import_dict['Publication Date'] = date.text\n",
    "            if date_type == \"Coverage\":  \n",
    "                match = re.search(r'>(\\d{4}-\\d{2}-\\d{2})/(\\d{4}-\\d{2}-\\d{2})<', date.text)\n",
    "                if match:\n",
    "                    coverage_start = match.group(1)\n",
    "                    coverage_end = match.group(2)\n",
    "                \n",
    "                    import_dict['Start Date'] = coverage_start\n",
    "                    import_dict['End Date'] = coverage_end\n",
    "                else:\n",
    "                    print(\"No dates found in the string.\")\n",
    "    #Publication year\n",
    "    for year in root.findall('{https://datacite.org/schema/kernel-4}publicationYear'):\n",
    "        import_dict['Publication Year'] = year.text\n",
    "        \n",
    "    \n",
    "    #### PUBLISHER \n",
    "    for publisher in root.findall('{https://datacite.org/schema/kernel-4}publisher'):\n",
    "        import_dict['Pub_Name'] = publisher.text\n",
    "        \n",
    "    #### DOCUMENTATION\n",
    "    \n",
    "    for descs in root.findall('{https://datacite.org/schema/kernel-4}descriptions'):\n",
    "        for desc in descs.findall('{https://datacite.org/schema/kernel-4}description'):\n",
    "            if desc.get('descriptionType') == \"Abstract\":\n",
    "                desc_text = desc.text\n",
    "                for summary in desc.iter():\n",
    "                    desc_text += summary.tail\n",
    "                    text = summary.tail\n",
    "                    if text.split(':')[0] == \"Summary\":\n",
    "                        # Using the first sentense of the summary section for the Abstract\n",
    "                        abstract_text = tokenize.sent_tokenize(text.split(':', 1)[1])[0]\n",
    "                        if len(abstract_text) > 180:\n",
    "                            abstract_text = abstract_text.split(')')[0]\n",
    "                        import_dict['Abstract'] = abstract_text\n",
    "                        \n",
    "                import_dict['Description'] = desc_text \n",
    "    \n",
    "    #### Coverage\n",
    "    for geo_locs in root.findall('{https://datacite.org/schema/kernel-4}geoLocations'):\n",
    "        for geo_loc in geo_locs.findall('{https://datacite.org/schema/kernel-4}geoLocation'):\n",
    "            for geo_loc_place in geo_loc.findall('{https://datacite.org/schema/kernel-4}geoLocationPlace'):\n",
    "                import_dict['Spatial Coverage'] = geo_loc_place.text\n",
    "            for geo_loc_bbox in geo_loc.findall('{https://datacite.org/schema/kernel-4}geoLocationBox'):\n",
    "                for west_long in geo_loc_bbox.findall('{https://datacite.org/schema/kernel-4}westBoundLongitude'):\n",
    "                    import_dict['Upper Right Longitude'] = west_long.text\n",
    "                for east_long in geo_loc_bbox.findall('{https://datacite.org/schema/kernel-4}eastBoundLongitude'):\n",
    "                    import_dict['Lower Left Longitude'] = east_long.text\n",
    "                for south_lat in geo_loc_bbox.findall('{https://datacite.org/schema/kernel-4}southBoundLatitude'):\n",
    "                    import_dict['Lower Left Latitude'] = south_lat.text\n",
    "                for north_lat in geo_loc_bbox.findall('{https://datacite.org/schema/kernel-4}northBoundLatitude'):\n",
    "                    import_dict['Upper Right Latitude'] = north_lat.text\n",
    "            \n",
    "    \n",
    "    #### USAGE\n",
    "    \n",
    "    # License\n",
    "    for rights in root.findall('{https://datacite.org/schema/kernel-4}rightsList'):\n",
    "        for right in rights.findall('{https://datacite.org/schema/kernel-4}rights'):\n",
    "            import_dict['License'] = right.text\n",
    "    \n",
    "    # Resource Creator\n",
    "    name_list = []\n",
    "    for creators in root.findall('{https://datacite.org/schema/kernel-4}creators'):\n",
    "        for creator in creators.findall('{https://datacite.org/schema/kernel-4}creator'):\n",
    "            for creator_name in creator.findall('{https://datacite.org/schema/kernel-4}creatorName'):\n",
    "                name_list.append(creator_name.text)\n",
    "    import_dict['Resource Creator'] = name_list\n",
    "    \n",
    "    #### Access\n",
    "    \n",
    "    # Access URL - see above section DOI name, or alternative indentifiers\n",
    "    \n",
    "    # language\n",
    "    for language in root.findall('{https://datacite.org/schema/kernel-4}language'):\n",
    "        import_dict['Language'] = language.text\n",
    "        \n",
    "    # format\n",
    "    format_list = []\n",
    "    for data_formats in root.findall('{https://datacite.org/schema/kernel-4}formats'):\n",
    "        for data_format in data_formats.findall('{https://datacite.org/schema/kernel-4}format'):\n",
    "            format_list.append(data_format.text)\n",
    "    import_dict['Format'] = format_list\n",
    "    \n",
    "    # references\n",
    "    for reference in root.findall('{https://datacite.org/schema/kernel-4}relatedIdentifiers'):\n",
    "        reference_list = []\n",
    "        referenced_by_list = []\n",
    "        qualified_relation_list = []\n",
    "        for ri in reference.findall('{https://datacite.org/schema/kernel-4}relatedIdentifier'):\n",
    "            rel_type = ri.attrib['relationType']\n",
    "            if rel_type in references_rel_types:\n",
    "                reference_list.append(\"https://doi.org/\" + ri.text)\n",
    "            elif rel_type in referencedby_rel_types:\n",
    "                referenced_by_list.append(\"https://doi.org/\" + ri.text)\n",
    "            else:\n",
    "                qualified_relation_list.append(\"https://doi.org/\" + ri.text)\n",
    "        import_dict['References'] = reference_list\n",
    "        import_dict['Is Referenced By'] = referenced_by_list\n",
    "        import_dict['Qualified Relation'] = qualified_relation_list\n",
    "    \n",
    "    #For fields with no match:\n",
    "    import_dict['License'] = ''\n",
    "    import_dict['Access Service']=''\n",
    "    import_dict['Purpose'] = ''\n",
    "    import_dict['Pub_Contact Point'] = 'data@dkrz.de'\n",
    "    import_dict['Associated Media'] = ''\n",
    "    import_dict['Is Part Of'] = ''\n",
    "    import_dict['Tools'] = ''\n",
    "    import_dict['Temporal Resolution']= '' \n",
    "    # import_dict['Start Date'] = ''\n",
    "    import_dict['Source'] = ''\n",
    "    import_dict['Spatial Resolution'] = ''\n",
    "    # import_dict['End Date'] = ''\n",
    "    import_dict['Investigations'] = '' \n",
    "    import_dict['Contact Point'] = 'data@dkrz.de'\n",
    "    import_dict['Pub_Identifier'] = 'https://ror.org/03ztgj037'\n",
    "    import_dict['Pub_Description'] = '' \n",
    "    import_dict['Pub_Logo'] = ''\n",
    "    import_dict['Spatial Aggregation'] = '' \n",
    "    import_dict['Jurisdiction'] = ''\n",
    "\n",
    "    import_list.append(import_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# this script creates a dataframe in the same order as the MDW Bulk Import sheet\n",
    "\n",
    "df = pd.DataFrame(import_list)\n",
    "column_order = ['Title', 'Abstract', 'Contact Point', 'Keywords', 'DOI Name', 'Alternate Identifier',\n",
    "                'Publication Date', 'Pub_Identifier', 'Pub_Name', 'Pub_Logo', 'Pub_Description', 'Pub_Contact Point', \n",
    "                'Description', 'Associated Media', 'Is Part Of', 'Spatial Coverage', 'Spatial Aggregation',\n",
    "                'Spatial Resolution', 'Start Date', 'End Date', 'Temporal Resolution', \n",
    "                'Lower Left Latitude', 'Lower Left Longitude', 'Upper Right Latitude', 'Upper Right Longitude',\n",
    "                'Purpose', 'Source', 'License', 'Resource Creator', 'Investigations',\n",
    "                'Is Referenced By', 'References', 'Access URL', 'Access Service',\n",
    "                'Jurisdiction', 'Language', 'Format', 'Qualified Relation', 'Tools']\n",
    "for col in column_order:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Warning: Column '{col}' not found. Adding it with NaN values.\")\n",
    "        df[col] = None # Or pd.NA, np.nan, '' depending on your data type preference\n",
    "\n",
    "# column_order = ['Contact Point', 'Keywords', 'DOI Name', 'Alternate Identifier',\n",
    "#                 'Publication Date', 'Pub_Identifier', 'Pub_Logo', 'Pub_Description', 'Pub_Contact Point', \n",
    "#                  'Associated Media', 'Is Part Of',  'Spatial Aggregation',\n",
    "#                 'Spatial Resolution',  'Start Date', 'End Date', 'Temporal Resolution', \n",
    "#                  'Purpose', 'Source', 'License', 'Resource Creator', 'Investigations',\n",
    "#                  'Access URL', 'Access Service',\n",
    "#                 'Jurisdiction',  'Format',  'Tools']\n",
    "df = df[column_order]\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Clean up\n",
    "\n",
    "This section of the code converts python lists to JSON.\n",
    "\n",
    "It replaces empty lists with NaN.  Note: as per the Readme, before the excel spreadsheet can be imported into the MDX, you will need to Find all \"NaN\" and replace with a blank cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def json_clean_column(a_pandas_series):\n",
    "    \n",
    "    # remove blanks\n",
    "    a_pandas_series = a_pandas_series.apply(lambda x: np.nan if len(x) == 0 else x)\n",
    "    \n",
    "    #replace single quotes with double quotes\n",
    "    a_pandas_series = a_pandas_series.apply(lambda x: json.dumps(x))\n",
    "    \n",
    "    return a_pandas_series"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# columns_with_lists = ['Keywords', 'Resource Creator',  'Is Referenced By', 'References', \n",
    "#                       'Format', 'Qualified Relation']\n",
    "columns_with_lists = ['Keywords', 'Resource Creator',  'Format']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in columns_with_lists:\n",
    "    df[col] = json_clean_column(df[col])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "date = datetime.date(datetime.now())\n",
    "print(date)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# This section of the code divides the records into seperate files containing no more than \n",
    "# n records per file for easy uploading\n",
    "\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "file_number = 1\n",
    "for l_df in list_df:\n",
    "    with pd.ExcelWriter('Output2025/DKRZ_metadata_v1_{}_fileno_{}.xlsx'.format(date, file_number)) as writer:  \n",
    "        l_df.to_excel(writer, sheet_name='DKRZ_Datasets', index=False)\n",
    "    file_number +=1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
